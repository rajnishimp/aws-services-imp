{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be093573b3ea41d8a840e14689708f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 20 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "20/07/05 06:15:13 INFO RSCDriver: Connecting to: ip-172-31-88-63.ec2.internal:10000\n",
      "20/07/05 06:15:13 INFO RSCDriver: Starting RPC server...\n",
      "20/07/05 06:15:13 INFO RpcServer: Connected to the port 10001\n",
      "20/07/05 06:15:13 WARN RSCConf: Your hostname, ip-172-31-88-63.ec2.internal, resolves to a loopback address, but we couldn't find any external IP address!\n",
      "20/07/05 06:15:13 WARN RSCConf: Set livy.rsc.rpc.server.address if you need to bind to another address.\n",
      "20/07/05 06:15:14 INFO RSCDriver: Received job request c950129b-ca1e-44a4-90f6-f4891c29c0d7\n",
      "20/07/05 06:15:14 INFO RSCDriver: SparkContext not yet up, queueing job request.\n",
      "20/07/05 06:15:22 INFO SparkEntries: Starting Spark context...\n",
      "20/07/05 06:15:22 INFO SparkContext: Running Spark version 2.4.5-amzn-0\n",
      "20/07/05 06:15:22 INFO SparkContext: Submitted application: livy-session-20\n",
      "20/07/05 06:15:22 INFO SecurityManager: Changing view acls to: livy\n",
      "20/07/05 06:15:22 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/07/05 06:15:22 INFO SecurityManager: Changing view acls groups to: \n",
      "20/07/05 06:15:22 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/07/05 06:15:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/07/05 06:15:22 INFO Utils: Successfully started service 'sparkDriver' on port 46277.\n",
      "20/07/05 06:15:22 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/07/05 06:15:22 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/07/05 06:15:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/07/05 06:15:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/07/05 06:15:22 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-46d217d8-16cc-49a1-beea-67473f8d9179\n",
      "20/07/05 06:15:22 INFO MemoryStore: MemoryStore started with capacity 1028.8 MB\n",
      "20/07/05 06:15:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/07/05 06:15:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/07/05 06:15:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-31-88-63.ec2.internal:4040\n",
      "20/07/05 06:15:23 INFO SparkContext: Added JAR file:/usr/lib/livy/rsc-jars/livy-api-0.7.0-incubating.jar at spark://ip-172-31-88-63.ec2.internal:46277/jars/livy-api-0.7.0-incubating.jar with timestamp 1593929723575\n",
      "20/07/05 06:15:23 INFO SparkContext: Added JAR file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.0-incubating.jar at spark://ip-172-31-88-63.ec2.internal:46277/jars/livy-rsc-0.7.0-incubating.jar with timestamp 1593929723576\n",
      "20/07/05 06:15:23 INFO SparkContext: Added JAR file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar at spark://ip-172-31-88-63.ec2.internal:46277/jars/netty-all-4.1.17.Final.jar with timestamp 1593929723577\n",
      "20/07/05 06:15:23 INFO SparkContext: Added JAR file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar at spark://ip-172-31-88-63.ec2.internal:46277/jars/commons-codec-1.9.jar with timestamp 1593929723577\n",
      "20/07/05 06:15:23 INFO SparkContext: Added JAR file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.7.0-incubating.jar at spark://ip-172-31-88-63.ec2.internal:46277/jars/livy-core_2.11-0.7.0-incubating.jar with timestamp 1593929723577\n",
      "20/07/05 06:15:23 INFO SparkContext: Added JAR file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.7.0-incubating.jar at spark://ip-172-31-88-63.ec2.internal:46277/jars/livy-repl_2.11-0.7.0-incubating.jar with timestamp 1593929723578\n",
      "20/07/05 06:15:23 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/07/05 06:15:24 INFO RMProxy: Connecting to ResourceManager at ip-172-31-88-63.ec2.internal/172.31.88.63:8032\n",
      "20/07/05 06:15:25 INFO Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "20/07/05 06:15:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "20/07/05 06:15:25 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "20/07/05 06:15:25 INFO Client: Setting up container launch context for our AM\n",
      "20/07/05 06:15:25 INFO Client: Setting up the launch environment for our AM container\n",
      "20/07/05 06:15:25 INFO Client: Preparing resources for our AM container\n",
      "20/07/05 06:15:25 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/07/05 06:15:29 INFO Client: Uploading resource file:/mnt/tmp/spark-ddd76ead-2182-4eb0-96e2-8b2db6f801cf/__spark_libs__1343786679280206597.zip -> hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984/__spark_libs__1343786679280206597.zip\n",
      "20/07/05 06:15:30 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.0-incubating.jar -> hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984/livy-api-0.7.0-incubating.jar\n",
      "20/07/05 06:15:30 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.0-incubating.jar -> hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984/livy-rsc-0.7.0-incubating.jar\n",
      "20/07/05 06:15:30 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984/netty-all-4.1.17.Final.jar\n",
      "20/07/05 06:15:31 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984/commons-codec-1.9.jar\n",
      "20/07/05 06:15:31 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.7.0-incubating.jar -> hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984/livy-core_2.11-0.7.0-incubating.jar\n",
      "20/07/05 06:15:31 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.7.0-incubating.jar -> hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984/livy-repl_2.11-0.7.0-incubating.jar\n",
      "20/07/05 06:15:31 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984/sparkr.zip\n",
      "20/07/05 06:15:31 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984/pyspark.zip\n",
      "20/07/05 06:15:31 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984/py4j-0.10.7-src.zip\n",
      "20/07/05 06:15:31 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/07/05 06:15:31 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/07/05 06:15:31 INFO Client: Uploading resource file:/mnt/tmp/spark-ddd76ead-2182-4eb0-96e2-8b2db6f801cf/__spark_conf__2607000170715691427.zip -> hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984/__spark_conf__.zip\n",
      "20/07/05 06:15:31 INFO SecurityManager: Changing view acls to: livy\n",
      "20/07/05 06:15:31 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/07/05 06:15:31 INFO SecurityManager: Changing view acls groups to: \n",
      "20/07/05 06:15:31 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/07/05 06:15:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/07/05 06:15:34 INFO Client: Submitting application application_1592730717036_40984 to ResourceManager\n",
      "20/07/05 06:15:35 INFO Client: Deleted staging directory hdfs://ip-172-31-88-63.ec2.internal:8020/user/livy/.sparkStaging/application_1592730717036_40984\n",
      "20/07/05 06:15:35 ERROR SparkContext: Error initializing SparkContext.\n",
      "org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1592730717036_40984 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 10000 applications, cannot accept submission of application: application_1592730717036_40984\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:276)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:183)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:183)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2521)\n",
      "\tat org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "\tat org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "\tat org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "\tat org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "\tat org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "\tat org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/07/05 06:15:35 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-88-63.ec2.internal:4040\n",
      "20/07/05 06:15:35 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "20/07/05 06:15:35 INFO YarnClientSchedulerBackend: Stopped\n",
      "20/07/05 06:15:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/07/05 06:15:35 INFO MemoryStore: MemoryStore cleared\n",
      "20/07/05 06:15:35 INFO BlockManager: BlockManager stopped\n",
      "20/07/05 06:15:35 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/07/05 06:15:35 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n",
      "20/07/05 06:15:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/07/05 06:15:35 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/07/05 06:15:35 WARN RSCDriver: Error during cancel job.\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.livy.rsc.driver.JobWrapper.cancel(JobWrapper.java:90)\n",
      "\tat org.apache.livy.rsc.driver.RSCDriver.shutdown(RSCDriver.java:128).\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "case class User(mailid: String,mobilenumber: String);\n",
    "val rdd = sc.textFile(\"s3://userrawdataintake/user.txt\");// extract data from s3,kafka,hdfs,DB,FTP,MQ,parquet,csv .. \n",
    "val df=rdd.map(_.split(\",\")).map(attributes => User(attributes(0), attributes(1))).toDF();// tranformation\n",
    "//df.write.parquet(\"hdfs://ip-172-31-88-63.ec2.internal:8020/user/user.parquet\") // load\n",
    "df.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://ip-172-31-88-63.ec2.internal:8020/user/user.parquet\") // load\n",
    "df.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bc1023232d41afa4622e809ef9b0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined class User\n",
      "parquetUserDF: org.apache.spark.sql.DataFrame = [mailid: string, mobilenumber: string]\n",
      "df: org.apache.spark.sql.DataFrame = [mail: string, count: bigint]\n",
      "+----------+-----+\n",
      "|      mail|count|\n",
      "+----------+-----+\n",
      "| yahoo.com|    3|\n",
      "| gmail.com|    6|\n",
      "|rediff.com|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case class User(mail: String,mobilenumber: String);\n",
    "val parquetUserDF = spark.read.parquet(\"hdfs://ip-172-31-88-63.ec2.internal:8020/user/user.parquet\");// extract data from s3,kafka,hdfs,DB,FTP,MQ,parquet,csv .. \n",
    "val df=parquetUserDF.map(attributes => User(attributes(0).asInstanceOf[String].split(\"@\")(1), attributes(1).asInstanceOf[String])).groupBy(\"mail\").count();// tranformation\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"s3://s3file3/usermaildomain.csv\");\n",
    "//df.write.csv(\"s3://s3file3/usermaildomain.csv\") // load\n",
    "df.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
